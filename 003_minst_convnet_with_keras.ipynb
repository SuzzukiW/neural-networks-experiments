{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MINST Convnet Model"
      ],
      "metadata": {
        "id": "xV2mdDVBQaxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, we first load the MNIST dataset and reshape the input data to have a depth of 1 (i.e., grayscale images). We also convert the target labels to one-hot encoding.\n",
        "\n",
        "Next, we define a Sequential model and add the following layers:\n",
        "\n",
        "- Conv2D layer with 32 filters of size (3,3), ReLU activation, and input shape of (28, 28, 1)\n",
        "- MaxPooling2D layer with pool size (2,2)\n",
        "- Flatten layer to convert the 2D feature maps to 1D feature vectors\n",
        "- Dense layer with 128 neurons and ReLU activation\n",
        "- Dense layer with 10 neurons (one for each class) and softmax activation\n",
        "\n",
        "After all that, we compile the model using categorical cross-entropy loss and the Adam optimizer, and train the model for 10 epochs with a batch size of 128. We also evaluate the model on the test set and print the test loss and accuracy."
      ],
      "metadata": {
        "id": "VQOQECGaSKE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Model"
      ],
      "metadata": {
        "id": "cvo2fuhgSyBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heVeNK7AQQsI",
        "outputId": "91c7e42d-c28d-493b-cc5c-01fcbd095d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 50s 99ms/step - loss: 0.8986 - accuracy: 0.9261 - val_loss: 0.1257 - val_accuracy: 0.9665\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 38s 80ms/step - loss: 0.0856 - accuracy: 0.9766 - val_loss: 0.0952 - val_accuracy: 0.9743\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 33s 71ms/step - loss: 0.0497 - accuracy: 0.9852 - val_loss: 0.0816 - val_accuracy: 0.9802\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 34s 73ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0801 - val_accuracy: 0.9789\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 35s 75ms/step - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.0923 - val_accuracy: 0.9761\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 35s 75ms/step - loss: 0.0253 - accuracy: 0.9921 - val_loss: 0.0957 - val_accuracy: 0.9774\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 34s 72ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.1017 - val_accuracy: 0.9786\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 33s 70ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1129 - val_accuracy: 0.9797\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 34s 72ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.0960 - val_accuracy: 0.9811\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 32s 68ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.0902 - val_accuracy: 0.9802\n",
            "Test loss: 0.09015689045190811\n",
            "Test accuracy: 0.9801999926567078\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data to have a depth of 1 (i.e., grayscale images)\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Convert the target labels to one-hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified Version"
      ],
      "metadata": {
        "id": "U1wR9regS0Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this modified version, we added more Conv2D layers, BatchNormalization layers, and Dropout layers to the simple MNIST convnet. We also increased the number of filters in each Conv2D layer and used Data Augmentation to generate new training images. We trained the model with Data Augmentation for 20 epochs and evaluated it on the test set."
      ],
      "metadata": {
        "id": "8dPSxAYcdKPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data to have a depth of 1 (i.e., grayscale images)\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Convert the target labels to one-hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f4KYw_eS3Pd",
        "outputId": "4a7ece6e-59bd-4d89-c262-a06ecb29a92c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first Conv2D layer with BatchNormalization and Dropout\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add the second Conv2D layer with BatchNormalization and Dropout\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add the third Conv2D layer with BatchNormalization and Dropout\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Add the MaxPooling2D layer and the Flatten layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add the first Dense layer with BatchNormalization and Dropout\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the second Dense layer with softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MAGvvlMUU8bB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Data Augmentation to generate new training images\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        zoom_range=0.1,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Train the model with Data Augmentation\n",
        "model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
        "                    steps_per_epoch=len(X_train) / 128, epochs=20, verbose=1,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "piSNmdr-VFwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU Activation Function"
      ],
      "metadata": {
        "id": "9W8aRrZkdv0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we come with this new activation function we used in this experiment.\n",
        "\n",
        "The rectified linear unit (ReLU) activation function is a commonly used activation function for neural networks, including convolutional neural networks (CNNs) like the ones we used in image recognition tasks such as the MNIST dataset."
      ],
      "metadata": {
        "id": "NU2ur1Jwd-FG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU function is defined as follows:\n",
        "\n",
        "```\n",
        "f(x) = max(0, x)\n",
        "```\n",
        "\n",
        "In other words, the output of the function is zero if the input is negative, and the input itself if it is positive.\n",
        "\n",
        "The ReLU function is simple, computationally efficient, and has been shown to work well in practice. It also helps to address the vanishing gradient problem that can occur with other activation functions like the sigmoid function.\n",
        "\n",
        "In the simple MNIST convnet that we built earlier, we used the ReLU activation function for the Conv2D layers and the Dense layers. This allows the model to learn non-linear relationships between the input features and the target labels, which is essential for tasks like image recognition."
      ],
      "metadata": {
        "id": "0-OQRiaDeLBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "uw2todFzeq1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we all know, data preprocessing is a critical step in building any machine learning model, including deep learning models.\n",
        "\n",
        "In general, data preprocessing refers to the process of transforming raw data into a format that is suitable for a machine learning model to consume."
      ],
      "metadata": {
        "id": "C30J3Y0ces_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, the MNIST dataset, the input data consists of 28x28 grayscale images of handwritten digits.\n",
        "\n",
        "To preprocess the data, we perform the following steps:\n",
        "\n",
        "- **Reshape the input data:** The input images are reshaped into a 4D tensor with dimensions (number of images, height, width, depth). In our case, since the images are grayscale, the depth is 1.\n",
        "\n",
        "- **Normalize the pixel values:** The pixel values of the input images are typically in the range of 0 to 255. To improve the performance of the model and prevent issues like overfitting, we typically normalize the pixel values to be in the range of 0 to 1. This is done by dividing each pixel value by 255."
      ],
      "metadata": {
        "id": "oAKMZAFFfBvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the sample code:"
      ],
      "metadata": {
        "id": "zla0v-mgfZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input data to have a depth of 1 (i.e., grayscale images)\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Convert the target labels to one-hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Convert the pixel values to floats and normalize them to the range of 0 to 1\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEc36QKXfcaW",
        "outputId": "8164646a-82fa-4445-aa8b-edd49c6cc59c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first load the MNIST dataset using the ```mnist.load_data()``` function from Keras. We then reshape the input data to have a depth of 1 using the reshape() function. Next, we convert the target labels to one-hot encoding using the np_utils.to_categorical() function. Finally, we convert the pixel values to floats and normalize them to the range of 0 to 1 by dividing by 255.\n"
      ],
      "metadata": {
        "id": "fsCbg41Mfvs_"
      }
    }
  ]
}