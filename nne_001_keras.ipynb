{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLZDk59Weql"
      },
      "source": [
        "# Neural Network Experiments 001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJcOjtyBV-1_"
      },
      "source": [
        "To start, we'll import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j69NhW7wV1zm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NwRRVjxWB14"
      },
      "source": [
        "Next, we'll define the input and output data for our neural network. In this example, we will use a dataset of student exam scores, where the input data is the scores for two exams, and the output data is whether the student was admitted to a university or not. We'll create some random data for demonstration purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "afed852tV848"
      },
      "outputs": [],
      "source": [
        "# Generate some random data for demonstration purposes\n",
        "X = np.random.rand(100, 2)\n",
        "y = np.random.randint(2, size=(100, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u-uk1rQWLC9"
      },
      "source": [
        "Now, we'll create our neural network using Keras. Our neural network will have an input layer, a hidden layer with 4 neurons, and an output layer. We'll use the Sequential class from Keras to create our model, and then add layers to it using the add() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bL6ySL9eWKX7"
      },
      "outputs": [],
      "source": [
        "# Create a neural network with one hidden layer of 4 neurons\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G2Bd8deWcXq"
      },
      "source": [
        "The Dense layer in Keras is a fully connected layer, where each neuron in one layer is connected to every neuron in the next layer.\n",
        "\n",
        "The input_dim parameter specifies the number of input features, which in this case is 2 (the exam scores).\n",
        "\n",
        "The activation parameter specifies the activation function for the neurons in the layer, which in this case is the rectified linear unit (ReLU) function for the hidden layer and the sigmoid function for the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s78fZ2fBXFYv"
      },
      "source": [
        "Now that we have defined our model, we need to compile it. Compiling the model involves specifying the loss function, optimizer, and metrics. In this example, we will use **binary cross-entropy** as the loss function, stochastic gradient descent as the optimizer, and accuracy as the metric to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PHv0c6ISXO2L"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zfcpF1XXSev"
      },
      "source": [
        "Next, we'll train our model on the input and output data. We'll use the fit() method to train the model, and specify the number of epochs (iterations) and batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x95FsYSGXavy",
        "outputId": "bd3b289b-2a82-41bf-bc9b-8b0f6f73f102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 4s 3ms/step - loss: 0.7013 - accuracy: 0.5200\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7011 - accuracy: 0.5300\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7014 - accuracy: 0.5100\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7008 - accuracy: 0.5200\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7007 - accuracy: 0.5200\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7001 - accuracy: 0.5200\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7000 - accuracy: 0.5200\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.5200\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6995 - accuracy: 0.5200\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6993 - accuracy: 0.5200\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6992 - accuracy: 0.5200\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6988 - accuracy: 0.5200\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.5200\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.5200\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6975 - accuracy: 0.5200\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.5200\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.5200\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.5200\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.5200\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.5200\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.5200\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6953 - accuracy: 0.5200\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6953 - accuracy: 0.5200\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.5200\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6947 - accuracy: 0.5200\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.5300\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.5200\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5200\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5300\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5200\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5100\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5200\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5100\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5200\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5100\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5100\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5100\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5200\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5100\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5300\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5400\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5600\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5700\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5700\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5500\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5600\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5500\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5500\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5500\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5600\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5600\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5500\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5500\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5600\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5700\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5700\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5700\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5500\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5800\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5800\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5800\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5800\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5600\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.5800\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5600\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5700\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5700\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5700\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5800\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5800\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5700\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5800\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5900\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5600\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5900\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5900\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5800\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5800\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5700\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5700\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5700\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5700\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5800\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5800\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5700\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5600\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5700\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5600\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5600\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.5600\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5700\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5800\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5600\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5800\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5800\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5700\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5800\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5700\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5700\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5800\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ae8086850>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJjzKjk-Xt4B"
      },
      "source": [
        "Finally, we can use the trained model to make predictions on new data. We'll create some new data for demonstration purposes, and use the predict() method to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCKO1xsTX2kk",
        "outputId": "3fe0fb43-dbdd-47a5-d27b-4100612070ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 143ms/step\n",
            "[[0.47026658]\n",
            " [0.43761432]\n",
            " [0.4869667 ]\n",
            " [0.49374872]\n",
            " [0.5150376 ]]\n"
          ]
        }
      ],
      "source": [
        "# Generate some new data for prediction\n",
        "X_new = np.random.rand(5, 2)\n",
        "\n",
        "# Use the trained model to make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOHDMCHX70d"
      },
      "source": [
        "That's it! This is a simple example of how to build and train a neural network using Keras in Python. Of course, there are many other things you can do with neural networks, including adding more layers, changing the activation functions, and using different optimization algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR69RuFQYVSU"
      },
      "source": [
        "## Adjusting the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eluhyoTYiyZ"
      },
      "source": [
        "The learning rate is a hyperparameter that controls the step size of the optimizer during training. If the learning rate is too high, the optimizer may overshoot the minimum and fail to converge. If the learning rate is too low, the optimizer may get stuck in a local minimum. You can adjust the learning rate by passing a different value to the lr parameter of the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHwCsQTLYUs1"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "# Create a neural network with one hidden layer of 4 neurons\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with a custom learning rate\n",
        "sgd = SGD(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZArC6d4YtNR"
      },
      "source": [
        "## Add more layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93CrJeCUYxf9"
      },
      "source": [
        "Adding more layers to a neural network can increase its capacity to learn complex patterns in the data. However, adding too many layers can cause the model to overfit, meaning it memorizes the training data rather than learning general patterns. You can add more layers to the model by calling the add() method multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrVk7cEpYz1j",
        "outputId": "5046eaa5-cb6f-4004-9551-b55305f577f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.4800\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.4900\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5100\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5300\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5500\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5700\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5600\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5300\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5300\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5100\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5300\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5300\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5200\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5400\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5500\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5100\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5400\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5100\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5300\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5500\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5300\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5500\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5400\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5800\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5700\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5700\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5800\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5800\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5900\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5900\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.6000\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5700\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5800\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.6100\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.6100\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5900\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5700\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.6100\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.6100\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.6200\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.6100\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.6000\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5900\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.6000\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.6100\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.6200\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.6200\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.6100\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5900\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5900\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5900\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5800\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.6000\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.6000\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5900\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5800\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.6000\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5800\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5600\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5800\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5700\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5800\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5600\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5800\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5600\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5700\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5700\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5900\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5700\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5800\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5800\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5600\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5700\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5600\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5700\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5600\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5700\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5600\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5700\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5800\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5600\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5600\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5600\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5700\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5600\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5700\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5900\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5700\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5600\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6859 - accuracy: 0.5700\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6859 - accuracy: 0.5700\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5600\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5700\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5700\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5600\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5700\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5600\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5700\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5700\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6860 - accuracy: 0.5700\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0adc4aa880>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a neural network with two hidden layers of 4 neurons each\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAb0h9EoZpW1"
      },
      "source": [
        "## Use a different activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kmC8AaUZqwT"
      },
      "source": [
        "The choice of activation function can have a big impact on the performance of the neural network. Different activation functions have different properties, and some are better suited for certain types of problems. You can use a different activation function by passing a different value to the activation parameter of the Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EiJov-fZq3u"
      },
      "outputs": [],
      "source": [
        "# Create a neural network with one hidden layer of 4 neurons with a tanh activation function\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
